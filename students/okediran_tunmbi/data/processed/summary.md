## Summary
To handle the hierarchical structure of the Xarray dataset extracted from XML, I aggregated the data by both horse and race_id. I applied the same preprocessing logic to the two prediction datasets to ensure consistency in format and structure. One of the main challenges I faced—aside from preparing the training dataset—was getting the model to run efficiently on my local machine. Initially, the training process was extremely slow, taking up to three hours. This was due to the absence of g++ on my system, which caused PyMC to rely solely on PyTensor for computation. Installing g++ and adding it to my system path, along with the troubleshooting involved, consumed a significant portion of my time. However, once that hurdle was resolved, the rest of the workflow became more straightforward.

The core of my approach involved building a classification model to predict the winner of a race using the BART (Bayesian Additive Regression Trees) framework. Although BART is traditionally designed for regression tasks, I adapted it for binary classification by modeling a latent function of the features and passing it through a logistic (sigmoid) function to obtain class probabilities. These probabilities represent the likelihood of a horse winning a race. I then placed a Bernoulli likelihood over the observed binary outcomes, enabling the model to infer posterior distributions over predicted probabilities. This Bayesian treatment not only supports robust classification but also provides a principled way to capture uncertainty in the predictions.

Overall, this was a highly educational and rewarding experience. What began as a frustrating technical obstacle ultimately became a motivating force that deepened my understanding of probabilistic modeling and hands-on problem solving.